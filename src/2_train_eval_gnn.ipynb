{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374070c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from typing import List, Dict, Tuple\n",
    "import io\n",
    "import sys\n",
    "import pickle\n",
    "import itertools\n",
    "import datetime\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as T\n",
    "import torch_geometric.transforms\n",
    "import torch_geometric.datasets\n",
    "import torch_geometric.nn\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_variables import *\n",
    "from utils_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f73a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_output_dir = \"./results\"\n",
    "if not os.path.exists(results_output_dir):\n",
    "    os.makedirs(results_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b9b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf221e7",
   "metadata": {},
   "source": [
    "# Deep Learning Pytorch\n",
    "Packages versions:\n",
    "- torchmetrics==0.5.0\n",
    "- pytorch-lightning==1.5.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dfb885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_constraints_func(probabilities, data, threshold=0.5, debug = False, constraints_type=\"greedy\"):\n",
    "    # Apply constraints to the predicted probabilities\n",
    "    edge_types = data.edge_types.detach().cpu().numpy()\n",
    "    source_nodes = data.edge_index[0].detach().cpu().numpy()\n",
    "    target_nodes = data.edge_index[1].detach().cpu().numpy()\n",
    "    allowed_edge_types = set([edges_type_int_encodings['nuclei-golgi'], edges_type_int_encodings['golgi-nuclei']])\n",
    "\n",
    "    if constraints_type==\"greedy\":\n",
    "        \"\"\"\n",
    "        Greedy Approach to apply constraints to the model output.\n",
    "        \"\"\"\n",
    "        # Sort edge_list_info and probabilities in descending order of probabilities\n",
    "        sorted_indices = sorted(range(len(probabilities)), key=lambda k: probabilities[k], reverse=True)\n",
    "        \n",
    "        edge_types_sorted = [edge_types[i] for i in sorted_indices]\n",
    "        source_nodes_sorted = [source_nodes[i] for i in sorted_indices]\n",
    "        target_nodes_sorted = [target_nodes[i] for i in sorted_indices]\n",
    "        \n",
    "        probabilities_sorted = [probabilities[i] for i in sorted_indices]\n",
    "\n",
    "        # Create a set to keep track of assigned node ids\n",
    "        assigned_nodes = set()\n",
    "\n",
    "        # Create a new list to store the predicted labels\n",
    "        pred_labels = [0] * len(probabilities)\n",
    "        \n",
    "        # Assign 1 to the links with the highest probabilities for each nuclei and golgi   \n",
    "        for i in range(len(sorted_indices)):\n",
    "            src = source_nodes_sorted[i]\n",
    "            tgt = target_nodes_sorted[i]\n",
    "            edge_type = edge_types_sorted[i]\n",
    "            prob = probabilities_sorted[i]\n",
    "            \n",
    "            # If the edge is nuclei-golgi or golgi-nuclei and the nodes are not already assigned and probability > threshold\n",
    "            if (edge_type in allowed_edge_types) and (src not in assigned_nodes) and (tgt not in assigned_nodes):\n",
    "\n",
    "                if prob > threshold:\n",
    "                    pred_labels[sorted_indices[i]] = 1\n",
    "                    assigned_nodes.add(src)\n",
    "                    assigned_nodes.add(tgt)\n",
    "    elif constraints_type==\"optimization\":\n",
    "        \"\"\"\n",
    "        Apply modified version of the Jonker-Volgenant algorithm to get global labels.\n",
    "        \"\"\"\n",
    "        # Build a NetworkX bipartite graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        G.add_nodes_from(source_nodes, bipartite=0)\n",
    "        G.add_nodes_from(target_nodes, bipartite=1)\n",
    "\n",
    "        # Invert probabilities\n",
    "        MAX_VALUE = 1e9  # A very large number\n",
    "        inverse_probabilities = [1 / prob if prob != 0 else MAX_VALUE for prob in probabilities]\n",
    "\n",
    "\n",
    "        # Add weighted edges with inverse probabilities\n",
    "        for i in range(len(source_nodes)):\n",
    "            src = source_nodes[i]\n",
    "            tgt = target_nodes[i]\n",
    "            edge_type = edge_types[i]\n",
    "            weight = inverse_probabilities[i]\n",
    "\n",
    "            # Add edge only if it's nuclei-golgi or golgi-nuclei\n",
    "            if edge_type in allowed_edge_types:\n",
    "                G.add_edge(src, tgt, weight=weight)\n",
    "\n",
    "        # Compute minimum weight full matching\n",
    "        matching = nx.bipartite.minimum_weight_full_matching(G, weight='weight')\n",
    "\n",
    "        # Create a dictionary to store predicted assignments\n",
    "        preds_dict = {}\n",
    "\n",
    "        # Assign labels based on matching\n",
    "        for src, tgt in matching.items():\n",
    "            preds_dict[src] = tgt\n",
    "            preds_dict[tgt] = src\n",
    "\n",
    "        # Initialize predicted labels\n",
    "        pred_labels = [0] * len(probabilities)\n",
    "        \n",
    "        # Assign labels based on matching dictionary\n",
    "        for i in range(len(source_nodes)):\n",
    "            src = source_nodes[i]\n",
    "            tgt = target_nodes[i]\n",
    "            \n",
    "            if preds_dict.get(src) == tgt or preds_dict.get(tgt) == src:\n",
    "                pred_labels[i] = 1\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Wrong constraints type!\")\n",
    "\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MPNN Classifier built from scratch\n",
    "\"\"\"\n",
    "\n",
    "from torch_scatter import scatter\n",
    "from typing import List, Any, Iterable\n",
    "\n",
    "def dims_from_multipliers(output_dim: int, multipliers: Iterable[int]) -> Tuple[int, ...]:\n",
    "    return tuple(int(output_dim * mult) for mult in multipliers)\n",
    "\n",
    "from torch.nn import LeakyReLU\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, fc_dims: Iterable[int], nonlinearity: torch.nn.Module,\n",
    "                 dropout_p: float = 0, use_batchnorm: bool = False, last_output_free: bool = False):\n",
    "        super().__init__()\n",
    "        assert isinstance(fc_dims, (list, tuple)\n",
    "                          ), f\"fc_dims must be a list or a tuple, but got {type(fc_dims)}\"\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.fc_dims = fc_dims\n",
    "        self.nonlinearity = nonlinearity\n",
    "        # if dropout_p is None:\n",
    "        #     dropout_p = 0\n",
    "        # if use_batchnorm is None:\n",
    "        # use_batchnorm = False\n",
    "        self.dropout_p = dropout_p\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "\n",
    "        layers: List[torch.nn.Module] = []\n",
    "        for layer_i, dim in enumerate(fc_dims):\n",
    "            layers.append(torch.nn.Linear(input_dim, dim))\n",
    "            if last_output_free and layer_i == len(fc_dims) - 1:\n",
    "                continue\n",
    "\n",
    "            layers.append(nonlinearity)\n",
    "            if dim != 1:\n",
    "                if use_batchnorm:\n",
    "                    layers.append(torch.nn.BatchNorm1d(dim))\n",
    "                if dropout_p > 0:\n",
    "                    layers.append(torch.nn.Dropout(p=dropout_p))\n",
    "            input_dim = dim\n",
    "        self.fc_layers = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layers(x)\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return self.fc_dims[-1]\n",
    "    \n",
    "class BasicEdgeModel(torch.nn.Module):\n",
    "    \"\"\" Class used to peform an edge update during neural message passing \"\"\"\n",
    "\n",
    "    def __init__(self, edge_mlp):\n",
    "        super(BasicEdgeModel, self).__init__()\n",
    "        self.edge_mlp = edge_mlp\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        source_nodes, target_nodes = edge_index\n",
    "        # assert len(source_nodes) == len(target_nodes) == len(\n",
    "            # edge_attr), f\"Different lengths {len(source_nodes)}, {len(target_nodes)}, {len(edge_attr)} \"\n",
    "        merged_features = torch.cat([x[source_nodes], x[target_nodes], edge_attr], dim=1)\n",
    "        # print(f\"merged_features, {merged_features.shape}\")\n",
    "        assert len(merged_features) == len(source_nodes), f\"Merged input has wrong length {merged_features.shape} != {edge_attr.shape}\"\n",
    "        return self.edge_mlp(merged_features)\n",
    "    \n",
    "class UniformAggNodeModel(torch.nn.Module):\n",
    "    \"\"\" Class used to peform a node update during neural message passing \"\"\"\n",
    "\n",
    "    def __init__(self, flow_model, node_mlp, node_agg_mode: str):\n",
    "        super().__init__()\n",
    "        assert (flow_model.output_dim == node_mlp.input_dim), f\"Flow models have incompatible output/input dims\"\n",
    "        self.flow_model = flow_model\n",
    "        self.node_mlp = node_mlp\n",
    "        self.node_agg_mode = node_agg_mode\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, **kwargs):\n",
    "        start_nodes, end_nodes = edge_index\n",
    "\n",
    "        \"\"\"\n",
    "        x[start_nodes]  # features of nodes emitting messages, past -> future\n",
    "        edge_attr       # emitted messages\n",
    "        x[end_nodes] # features of nodes receiving messages, future nodes receiving from earlier ones\n",
    "        \"\"\"\n",
    "        # input features order does not matter as long as it is symmetric between two flow inputs\n",
    "        #                               nodes receiving, nodes sending, edges\n",
    "        flow_forward_input = torch.hstack((x[end_nodes], x[start_nodes], edge_attr))\n",
    "        assert len(flow_forward_input) == len(edge_attr)\n",
    "        # [n_edges x edge_feature_count]\n",
    "        flow_backward_input = torch.hstack((x[start_nodes], x[end_nodes], edge_attr))\n",
    "        assert flow_forward_input.shape == flow_backward_input.shape, f\"{flow_forward_input.shape} != {flow_backward_input.shape}\"\n",
    "\n",
    "        # [2*n_edges x edge_feature_count]\n",
    "        flow_total_input = torch.vstack((flow_forward_input, flow_backward_input))\n",
    "        flow_processed = self.flow_model(flow_total_input)\n",
    "\n",
    "        # aggregate features for each node based on features taken over each node\n",
    "        # the index has to account for both incoming and outgoing edges - so that each edge is considered by both of its nodes\n",
    "        flow_total = scatter(src=flow_processed, index=torch.cat((end_nodes, start_nodes)),\n",
    "                             reduce=self.node_agg_mode, dim=0, dim_size=len(x))\n",
    "        \n",
    "        return self.node_mlp(flow_total)\n",
    "    \n",
    "class InitialUniformAggNodeModel(torch.nn.Module):\n",
    "    \"\"\" Class used to peform an initial update for empty nodes at the beginning of message passing\n",
    "    The initial features are simply a transformation of edge features and therefore depend largely on the graph structure\n",
    "    The aggregation logic is the same as for `UniformAggNodeModel` but without node features.\n",
    "\n",
    "    The abscense of node features means that transforming features before aggregation is the same as transforming the input edge features before the node update. Therefore, the transform is only applied after aggregation and instead the initial edge model is more powerful in comparison.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, node_mlp, node_agg_mode: str):\n",
    "        super().__init__()\n",
    "        self.node_mlp = node_mlp\n",
    "        self.node_agg_mode = node_agg_mode\n",
    "\n",
    "    def forward(self, node_attr, edge_index, edge_attr, num_nodes: int, **kwargs):\n",
    "        start_nodes, end_nodes = edge_index\n",
    "        flow_total = scatter(src=torch.vstack((edge_attr, edge_attr)),\n",
    "                             index=torch.cat((end_nodes, start_nodes)),\n",
    "                             reduce=self.node_agg_mode, dim=0, dim_size=num_nodes)\n",
    "        return self.node_mlp(flow_total)\n",
    "    \n",
    "class InitialUniformAggNodeModelWithNodeFeats(torch.nn.Module):\n",
    "    \"\"\" Class used to peform an initial update for empty nodes at the beginning of message passing\n",
    "    The initial features are simply a transformation of edge features and therefore depend largely on the graph structure\n",
    "    The aggregation logic is the same as for `UniformAggNodeModel` but without node features.\n",
    "\n",
    "    The abscense of node features means that transforming features before aggregation is the same as transforming the input edge features before the node update. Therefore, the transform is only applied after aggregation and instead the initial edge model is more powerful in comparison.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, node_mlp, node_agg_mode: str):\n",
    "        super().__init__()\n",
    "        self.node_mlp = node_mlp\n",
    "        self.node_agg_mode = node_agg_mode\n",
    "\n",
    "    def forward(self, node_attr, edge_index, edge_attr, num_nodes: int, **kwargs):\n",
    "        start_nodes, end_nodes = edge_index\n",
    "        flow_total = scatter(src=torch.vstack((edge_attr, edge_attr)),\n",
    "                             index=torch.cat((end_nodes, start_nodes)),\n",
    "                             reduce=self.node_agg_mode, dim=0, dim_size=num_nodes)\n",
    "        return self.node_mlp(torch.cat((node_attr,flow_total), dim=1))\n",
    "\n",
    "\n",
    "class MessagePassingNetworkRecurrent(torch.nn.Module):\n",
    "    def __init__(self, edge_model: torch.nn.Module, node_model: torch.nn.Module, steps: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            edge_model: an Edge Update model\n",
    "            node_model: an Node Update model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.edge_model = edge_model\n",
    "        self.node_model = node_model\n",
    "        self.steps = steps\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, num_nodes: int):\n",
    "        \"\"\"\n",
    "        Does a single node and edge feature vectors update.\n",
    "        Args:\n",
    "            x: node features matrix\n",
    "            edge_index: tensor with shape [2, M], with M being the number of edges, indicating nonzero entries in the\n",
    "            graph adjacency (i.e. edges)\n",
    "            edge_attr: edge features matrix (ordered by edge_index)\n",
    "        Returns: Updated Node and Edge Feature matrices\n",
    "        \"\"\"\n",
    "        for step in range(self.steps):\n",
    "            # Edge Update\n",
    "            edge_attr_mpn = self.edge_model(x, edge_index, edge_attr)\n",
    "\n",
    "            if step == self.steps - 1:\n",
    "                continue  # do not process nodes in the last step - only edge features are used for classification\n",
    "            # Node Update\n",
    "            x = self.node_model(x, edge_index, edge_attr_mpn)\n",
    "        return x, edge_attr_mpn\n",
    "\n",
    "class MessagePassingNetworkNonRecurrent(torch.nn.Module):\n",
    "    def __init__(self, edge_models: List[torch.nn.Module], node_models: List[torch.nn.Module], steps: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            edge_models: a list/tuple of callable Edge Update models\n",
    "            node_models: a list/tuple of callable Node Update models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert len(edge_models) == steps, f\"steps={steps} not equal edge models {len(edge_models)}\"\n",
    "        assert len(node_models) == steps - 1, f\"steps={steps} -1 not equal node models {len(node_models)}\"\n",
    "        self.edge_models = torch.nn.ModuleList(edge_models)\n",
    "        self.node_models = torch.nn.ModuleList(node_models)\n",
    "        self.steps = steps\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, num_nodes: int):\n",
    "        \"\"\"\n",
    "        Does a single node and edge feature vectors update.\n",
    "        Args:\n",
    "            x: node features matrix\n",
    "            edge_index: tensor with shape [2, M], with M being the number of edges, indicating nonzero entries in the\n",
    "            graph adjacency (i.e. edges)\n",
    "            edge_attr: edge features matrix (ordered by edge_index)\n",
    "        Returns: Updated Node and Edge Feature matrices\n",
    "        \"\"\"\n",
    "        edge_embeddings = []\n",
    "        for step, (edge_model, node_model) in enumerate(zip(self.edge_models, self.node_models.append(None))):\n",
    "            # Edge Update\n",
    "            edge_attr_mpn = edge_model(x, edge_index, edge_attr)\n",
    "            edge_embeddings.append(edge_attr_mpn)\n",
    "\n",
    "            if step == self.steps - 1:\n",
    "                continue  # do not process nodes in the last step - only edge features are used for classification\n",
    "            # Node Update\n",
    "            x = node_model(x, edge_index, edge_attr_mpn)\n",
    "        assert len(edge_embeddings) == self.steps, f\"Collected {len(edge_embeddings)} edge embeddings for {self.steps} steps\"\n",
    "        return x, edge_embeddings[-1]\n",
    "\n",
    "class GraphClassifierMPNN(torch.nn.Module):\n",
    "    def __init__(self, dataset_num_node_features, dataset_num_edge_features, is_recurrent=True, use_node_feats= False):\n",
    "        \"\"\" Top level model class holding all components necessary to perform classification on a graph\n",
    "        :param dataset_num_edge_features: number of edge features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        params_gnn = {'initial_edge_model_input_dim': dataset_num_edge_features,\n",
    "        'edge_dim': 16,\n",
    "        'fc_dims_initial_edge_model_multipliers': (1, 1),\n",
    "        'nonlinearity_initial_edge': LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        'fc_dims_initial_node_model_multipliers': (2, 4, 1),\n",
    "        'nonlinearity_initial_node': LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        'directed_flow_agg': 'max',\n",
    "        'fc_dims_edge_model_multipliers': (4, 1),\n",
    "        'nonlinearity_edge': LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        'fc_dims_directed_flow_model_multipliers': (2, 1),\n",
    "        'nonlinearity_directed_flow': LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        'fc_dims_total_flow_model_multipliers': (4, 2, 1),\n",
    "        'nonlinearity_total_flow': LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        'fc_dims_edge_classification_model_multipliers': (4, 2, 1),\n",
    "        'nonlinearity_edge_classification': LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        'use_batchnorm': False,\n",
    "        'mpn_steps': 4,\n",
    "        'is_recurrent': is_recurrent,\n",
    "        'use_node_feats':use_node_feats,\n",
    "        'node_dim_multiplier': 2}\n",
    "        \n",
    "        edge_dim = params_gnn[\"edge_dim\"]\n",
    "        node_dim = edge_dim * params_gnn[\"node_dim_multiplier\"]  # Have nodes hold 2x info of edges\n",
    "        use_batchnorm = params_gnn[\"use_batchnorm\"]\n",
    "\n",
    "        #build_edge_model\n",
    "        fc_dims_initial_edge = dims_from_multipliers(edge_dim, params_gnn[\"fc_dims_initial_edge_model_multipliers\"])\n",
    "        self.initial_edge_model = MLP(params_gnn[\"initial_edge_model_input_dim\"], fc_dims_initial_edge,\n",
    "                            params_gnn[\"nonlinearity_initial_edge\"], use_batchnorm=use_batchnorm)\n",
    "        \n",
    "        # Initial node model    \n",
    "        initial_node_agg_mode = params_gnn[\"directed_flow_agg\"]\n",
    "        fc_dims_initial_node = dims_from_multipliers(node_dim, params_gnn[\"fc_dims_initial_node_model_multipliers\"])\n",
    "        \n",
    "        if(not use_node_feats):#Do not use features to compute initial node embeddings\n",
    "            self.initial_node_model = InitialUniformAggNodeModel(MLP(edge_dim, fc_dims_initial_node,\n",
    "                                                                    params_gnn[\"nonlinearity_initial_node\"], use_batchnorm=use_batchnorm),\n",
    "                                                                initial_node_agg_mode)\n",
    "        else:\n",
    "            self.initial_node_model = InitialUniformAggNodeModelWithNodeFeats(MLP(dataset_num_node_features+edge_dim, fc_dims_initial_node,\n",
    "                                                                    params_gnn[\"nonlinearity_initial_node\"], use_batchnorm=use_batchnorm),\n",
    "                                                                initial_node_agg_mode)\n",
    "        # Edge classification model    \n",
    "        fc_dims_edge_classification_model = dims_from_multipliers(edge_dim, params_gnn[\"fc_dims_edge_classification_model_multipliers\"]) + (1,)\n",
    "        self.edge_classifier = MLP(edge_dim, fc_dims_edge_classification_model,\n",
    "                            params_gnn[\"nonlinearity_edge_classification\"], last_output_free=True)\n",
    "        \n",
    "        # Define models in MPN\n",
    "        edge_models, node_models = [], []\n",
    "        steps = params_gnn[\"mpn_steps\"]\n",
    "        assert steps > 1, \"Fewer than 2 MPN steps does not make sense as in that case nodes do not get a chance to update\"\n",
    "        is_recurrent = params_gnn[\"is_recurrent\"]\n",
    "        for step in range(steps):\n",
    "            # Edge model\n",
    "            edge_model_input = node_dim * 2 + edge_dim  # edge_dim * 5\n",
    "            fc_dims_edge = dims_from_multipliers(edge_dim, params_gnn[\"fc_dims_edge_model_multipliers\"])\n",
    "            edge_models.append(BasicEdgeModel(MLP(edge_model_input, fc_dims_edge, params_gnn[\"nonlinearity_edge\"], use_batchnorm=use_batchnorm)))\n",
    "\n",
    "            if step == steps - 1: # don't need a node update at the last step\n",
    "                continue\n",
    "\n",
    "            # Node model\n",
    "            flow_model_input = node_dim * 2 + edge_dim  # two nodes and their edge\n",
    "            fc_dims_directed_flow = dims_from_multipliers(node_dim, params_gnn[\"fc_dims_directed_flow_model_multipliers\"])\n",
    "            fc_dims_aggregated_flow = dims_from_multipliers(node_dim, params_gnn[\"fc_dims_total_flow_model_multipliers\"])\n",
    "            \n",
    "            node_agg_mode = params_gnn[\"directed_flow_agg\"]\n",
    "\n",
    "            individual_flow_model = MLP(flow_model_input, fc_dims_directed_flow,params_gnn[\"nonlinearity_directed_flow\"], use_batchnorm=use_batchnorm)\n",
    "            aggregated_flow_model = MLP(node_dim, fc_dims_aggregated_flow,params_gnn[\"nonlinearity_total_flow\"], use_batchnorm=use_batchnorm)\n",
    "            node_models.append(UniformAggNodeModel(individual_flow_model,aggregated_flow_model, node_agg_mode))\n",
    "\n",
    "            if is_recurrent:  # only one model to use at each step\n",
    "                break\n",
    "\n",
    "        if is_recurrent:\n",
    "            assert len(edge_models) == len(node_models) == 1\n",
    "            self.mpn_model = MessagePassingNetworkRecurrent(edge_models[0], node_models[0], steps)\n",
    "        else:\n",
    "            self.mpn_model = MessagePassingNetworkNonRecurrent(edge_models, node_models, steps)\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "    \n",
    "    def forward(self, data):\n",
    "        node_attr, edge_index, edge_attr, num_nodes = data.x, data.edge_index.long(), data.edge_attr, data.num_nodes\n",
    "\n",
    "        # Initial Edge embeddings with Null node embeddings\n",
    "        edge_attr = self.initial_edge_model(edge_attr)\n",
    "        \n",
    "        # Initial Node embeddings with Null original embeddings\n",
    "        x = self.initial_node_model(node_attr, edge_index, edge_attr, num_nodes, device=self.device)\n",
    "        assert len(x) == num_nodes\n",
    "        \n",
    "        x, final_edge_embeddings = self.mpn_model(x, edge_index, edge_attr, num_nodes)\n",
    "        return self.edge_classifier(final_edge_embeddings)\n",
    "    \n",
    "    def forward_graph(self, graph, criterion = None):\n",
    "        out = self.forward(graph.pyg_graph).view(-1)\n",
    "        loss = None\n",
    "        true = graph.pyg_graph.edge_label\n",
    "        if(criterion):\n",
    "            loss = criterion(out, true)\n",
    "        return out, loss, true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88661c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MLP Classifier built from scratch\n",
    "\"\"\"\n",
    "\n",
    "class GraphClassifierMLP(torch.nn.Module):\n",
    "    def __init__(self, dimensions):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(dimensions) - 1):\n",
    "            layers.append(torch.nn.Linear(dimensions[i], dimensions[i+1]))\n",
    "            layers.append(torch.nn.ReLU())  # You can use other activation functions here\n",
    "\n",
    "        # Remove the last ReLU layer\n",
    "        layers.pop()\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "    def forward_graph(self, graph, criterion = None):\n",
    "        out = self.forward(torch.from_numpy(graph.edge_x).to(torch.float)).view(-1)\n",
    "        true = torch.from_numpy(graph.edge_y)\n",
    "        loss = None\n",
    "        if(criterion):\n",
    "            loss = criterion(out, true)\n",
    "        return out, loss, true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper:\n",
    "    def __init__(self, model, params):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "    \n",
    "    def save(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump({'model': self.model, 'params': self.params}, f)\n",
    "        print(\"Model saved successfully.\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        model = data['model']\n",
    "        params = data['params']\n",
    "        return cls(model, params)\n",
    "\n",
    "    # Example usage:\n",
    "    # Instantiate your model\n",
    "    # model = YourModel()\n",
    "    # params = {'param1': value1, 'param2': value2, ...}\n",
    "\n",
    "    # Wrap the model\n",
    "    # model_wrapper = ModelWrapper(model, params)\n",
    "\n",
    "    # Save the model\n",
    "    # model_wrapper.save('model.pkl')\n",
    "\n",
    "    # Load the model\n",
    "    # loaded_model_wrapper = ModelWrapper.load('model.pkl')\n",
    "    # loaded_model = loaded_model_wrapper.model\n",
    "    # loaded_params = loaded_model_wrapper.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "## Functions to  train and evaluate neural network \n",
    "#################################################################################################\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "def train_link_predictor(\n",
    "    model, train_data, val_data, optimizer, criterion, n_epochs=100, debug = False,\n",
    "    early_stopper = None, scheduler = None, apply_constraints = True\n",
    "):\n",
    "    early_stopper = early_stopper\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        random.shuffle(train_data)\n",
    "        for graph in train_data:\n",
    "            optimizer.zero_grad()\n",
    "            out, loss, true = model.forward_graph(graph, criterion = criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if(debug):\n",
    "            if epoch % 10 == 0:\n",
    "                # Eval the model at the end of each Epoch\n",
    "                metrics = eval_link_predictor(model, val_data, criterion = criterion, apply_constraints = apply_constraints)\n",
    "                print(f\"Epoch: {epoch:03d}, Train Loss: {loss:.3f}, Metrics:\",metrics)\n",
    "                \n",
    "\n",
    "        if early_stopper:\n",
    "            if early_stopper.early_stop(metrics[\"loss\"]):             \n",
    "                break\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def aggregate_metrics_all(metrics_list, loss_criterion=None):\n",
    "    aggregated_metrics = {}\n",
    "    \n",
    "    aggregated_metrics[\"rouc_auc_score\"] = statistics.mean([metric[\"rouc_auc_score\"] for metric in metrics_list])\n",
    "    aggregated_metrics[\"rouc_auc_curve\"] = metrics_list[0][\"rouc_auc_curve\"]\n",
    "\n",
    "    # Aggregate loss if provided\n",
    "    if loss_criterion:\n",
    "        aggregated_metrics[\"loss\"] = torch.mean(torch.stack([metric_[\"loss\"] for metric_ in metrics_list]), dim=0)\n",
    "\n",
    "    \n",
    "    # Aggregate other metrics\n",
    "    metric_keys = [\"@best\", \"@0.5\", \"@constraints\", \"@constraints_opt\"]\n",
    "\n",
    "    for key in metric_keys:\n",
    "        aggregated_metrics[key] = {}    \n",
    "        aggregated_metrics[key][\"metrics\"] = aggregate_metrics([metric[key][\"metrics\"] for metric in metrics_list])\n",
    "        \n",
    "        sample_metric = metrics_list[0][key]\n",
    "        if(\"@constraints\" in sample_metric):\n",
    "            aggregated_metrics[key][\"@constraints\"] = {}\n",
    "            aggregated_metrics[key][\"@constraints\"][\"metrics\"] = aggregate_metrics([metric[key][\"@constraints\"][\"metrics\"] for metric in metrics_list])\n",
    "\n",
    "    return aggregated_metrics\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_link_predictor(model, train_data, test_data, criterion = None, plot_roc_curve=False, debug = False, \n",
    "                                 apply_constraints=True):\n",
    "    model.eval()\n",
    "    \n",
    "    #computed metrics -> \"acc\", \"precision\", \"recall\", \"tp\", fp\", \"tn\", \"fn\"\n",
    "    metrics_dict = {}\n",
    "    metrics_dict[\"individual_metrics\"] = {}#the metrics for each graph, key=graph_id->value=graph_metrics\n",
    "    metrics_dict[\"aggregated_metrics\"] = {}\n",
    "    \n",
    "    #Find optimal threshold from training data here:\n",
    "    optimal_thresholds_list = []\n",
    "\n",
    "    for i in range(len(train_data)):\n",
    "        graph = train_data[i]\n",
    "        graph_id = train_data[i].graph_id\n",
    "        tp_total_count = len(train_data[i].edge_list)\n",
    "\n",
    "        out, loss, true = model.forward_graph(graph, criterion = None)\n",
    "\n",
    "        out = out.sigmoid()\n",
    "        pred = out.cpu().numpy()\n",
    "\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(true, pred)\n",
    "        sensitivity = tpr\n",
    "        specificity = 1 - fpr\n",
    "        optimal_idx = np.argmax(sensitivity + specificity - 1)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        optimal_thresholds_list.append({\"size\":tp_total_count, \"thresh\":optimal_threshold})\n",
    "    \n",
    "    #-> Uncomment Here to compute optimal_threshold based on weighted average\n",
    "    #_total_size = sum([_v[\"size\"] for _v in optimal_thresholds_list])\n",
    "    #optimal_threshold = sum([_v[\"size\"]*_v[\"thresh\"] for _v in optimal_thresholds_list])/len(optimal_thresholds_list)/_total_size#weighted average of the threshold for each graph\n",
    "\n",
    "    #-> Uncomment Here to compute optimal_threshold based on average\n",
    "    optimal_threshold = statistics.mean(_v[\"thresh\"] for _v in optimal_thresholds_list)\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        graph = test_data[i]\n",
    "        graph_id = test_data[i].graph_id\n",
    "        tp_total_count = len(test_data[i].edge_list)\n",
    "        \n",
    "        metrics = {}\n",
    "        out, loss, true = model.forward_graph(graph, criterion = criterion)\n",
    "        if(criterion!=None):\n",
    "            metrics[\"loss\"] = loss\n",
    "        \n",
    "        out = out.sigmoid()\n",
    "        pred = out.cpu().numpy()\n",
    "        \n",
    "        if len(np.unique(pred))==1 or len(np.unique(true)) == 1:\n",
    "            rouc_auc_score = 0\n",
    "        else:\n",
    "            rouc_auc_score = round(roc_auc_score(true, pred), 3)\n",
    "        \n",
    "\n",
    "        metrics[\"rouc_auc_score\"] = rouc_auc_score\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(true, pred)\n",
    "        metrics[\"rouc_auc_curve\"] = {\"fpr\":fpr.tolist(), \"tpr\":tpr.tolist(), \"thresholds\":thresholds.tolist()}\n",
    "        \n",
    "        # 0.5 threshold\n",
    "        pred_labels_05 = (pred > 0.5).astype(int)\n",
    "        metrics[\"@0.5\"] = {}\n",
    "        metrics[\"@0.5\"][\"pred_labels\"] = pred_labels_05#save pred labels to make plot of predicted graph\n",
    "        metrics[\"@0.5\"][\"metrics\"] = eval_metrics(true, pred_labels_05, tp_total_count)\n",
    "\n",
    "\n",
    "        #Not needed anymore -> roc auc score is computed from training dataset above and not from test dataset!\n",
    "        \"\"\"\n",
    "        sensitivity = tpr\n",
    "        specificity = 1 - fpr\n",
    "        optimal_idx = np.argmax(sensitivity + specificity - 1)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate pred_labels_best with constraints if required\n",
    "        pred_labels_best = (pred > optimal_threshold).astype(int)\n",
    "        metrics[\"@best\"] = {}\n",
    "        metrics[\"@best\"][\"metrics\"] = eval_metrics(true, pred_labels_best, tp_total_count)\n",
    "        metrics[\"@best\"][\"pred_labels\"] = pred_labels_best\n",
    "        metrics[\"@best\"][\"optimal_threshold\"] = optimal_threshold\n",
    "        metrics[\"figures\"] = {}\n",
    "\n",
    "        if(apply_constraints):\n",
    "            # Apply constraints\n",
    "            pred_labels_constraints_05 = apply_constraints_func(pred, graph.pyg_graph, threshold=0.5, constraints_type=\"greedy\")  \n",
    "            metrics[\"@0.5\"][\"@constraints\"] = {}\n",
    "            metrics[\"@0.5\"][\"@constraints\"][\"pred_labels\"] = pred_labels_constraints_05\n",
    "            metrics[\"@0.5\"][\"@constraints\"][\"metrics\"] = eval_metrics(true, pred_labels_constraints_05, tp_total_count)\n",
    "\n",
    "            # Apply constraints with threshold\n",
    "            pred_labels_constraints_best = apply_constraints_func(pred, graph.pyg_graph, threshold=optimal_threshold, constraints_type=\"greedy\") \n",
    "            metrics[\"@best\"][\"@constraints\"] = {}\n",
    "            metrics[\"@best\"][\"@constraints\"][\"metrics\"] = eval_metrics(true, pred_labels_constraints_best, tp_total_count)\n",
    "            metrics[\"@best\"][\"@constraints\"][\"pred_labels\"] = pred_labels_constraints_best\n",
    "            \n",
    "            # Apply constraints without threshold\n",
    "            pred_labels_constraints = apply_constraints_func(pred, graph.pyg_graph, threshold=0, constraints_type=\"greedy\")  \n",
    "            metrics[\"@constraints\"] = {}\n",
    "            metrics[\"@constraints\"][\"metrics\"] = eval_metrics(true, pred_labels_constraints, tp_total_count)\n",
    "            metrics[\"@constraints\"][\"pred_labels\"]  = pred_labels_constraints\n",
    "\n",
    "            # Apply constraints without jonker-volgenant\n",
    "            pred_labels_constraints_opt = apply_constraints_func(pred, graph.pyg_graph, constraints_type=\"optimization\")  \n",
    "            metrics[\"@constraints_opt\"] = {}\n",
    "            metrics[\"@constraints_opt\"][\"metrics\"] = eval_metrics(true, pred_labels_constraints_opt, tp_total_count)\n",
    "            metrics[\"@constraints_opt\"][\"pred_labels\"]  = pred_labels_constraints_opt\n",
    "        \n",
    "        metrics[\"pred_edge_probabilities\"] = pred\n",
    "        metrics_dict[\"individual_metrics\"][graph_id]= metrics\n",
    "        test_data[i].metrics = metrics\n",
    "\n",
    "    metrics_dict[\"aggregated_metrics\"] = aggregate_metrics_all(list(metrics_dict[\"individual_metrics\"].values()), \n",
    "                                                               loss_criterion = criterion)\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b3ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_type, dataset_num_node_features, dataset_num_edge_features, dataset_num_total_features,\n",
    "               dataset_num_classes):\n",
    "    model = None\n",
    "\n",
    "    if(model_type==\"GNN_Classifier_Recurrent_WithoutNodeFeats\"):\n",
    "        model = GraphClassifierMPNN(dataset_num_node_features, dataset_num_edge_features, is_recurrent=True, use_node_feats=False)\n",
    "    elif(model_type==\"GNN_Classifier_NonRecurrent_WithoutNodeFeats\"):\n",
    "        model = GraphClassifierMPNN(dataset_num_node_features, dataset_num_edge_features, is_recurrent=False, use_node_feats=False)\n",
    "    elif(model_type==\"GNN_Classifier_Recurrent_WithNodeFeats\"):\n",
    "        model = GraphClassifierMPNN(dataset_num_node_features, dataset_num_edge_features, is_recurrent=True, use_node_feats=True)\n",
    "    elif(model_type==\"GNN_Classifier_NonRecurrent_WithNodeFeats\"):\n",
    "        model = GraphClassifierMPNN(dataset_num_node_features, dataset_num_edge_features, is_recurrent=False, use_node_feats=True)\n",
    "    elif(model_type ==\"MLP\"):\n",
    "        model_dims = (dataset_num_total_features, 100, 100, dataset_num_classes)\n",
    "        model = GraphClassifierMLP(model_dims)\n",
    "    else:\n",
    "        raise ValueError(\"Wrong type of classifier!\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "def schedule_training_GNN(job_parameters, model, graph_list_train, graph_list_val , debug = False):   \n",
    "    \n",
    "    dataset_num_classes = job_parameters[\"num_classes\"]\n",
    "    dataset_num_node_features = job_parameters[\"num_node_features\"]\n",
    "    dataset_num_edge_features = job_parameters[\"num_edge_features\"]\n",
    "    k_inter = job_parameters[\"knn_inter_nodes\"]\n",
    "    k_intra = job_parameters[\"knn_intra_nodes\"]\n",
    "    \n",
    "    lr = job_parameters[\"lr\"]\n",
    "    n_epochs = job_parameters[\"n_epochs\"]\n",
    "    device = job_parameters[\"device\"]\n",
    "    \n",
    "    criterion_switch = {\"BCEWithLogitsLoss\":torch.nn.BCEWithLogitsLoss}\n",
    "    criterion_function = criterion_switch[job_parameters[\"criterion\"]]\n",
    "    \n",
    "    if job_parameters[\"pos_weight\"]:\n",
    "        k_inter_mean = statistics.mean([g.k_inter for g in graph_list_train])\n",
    "        pos_weight = torch.tensor([max((k_inter_mean+k_intra*2-1)/1,1)])\n",
    "        criterion = criterion_function(pos_weight = pos_weight)\n",
    "    else:\n",
    "        criterion = criterion_function()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    early_stopper = job_parameters[\"early_stopper\"]\n",
    "    scheduler = job_parameters[\"scheduler\"]\n",
    "    \n",
    "    model = train_link_predictor(model, graph_list_train, graph_list_val, optimizer, criterion,\n",
    "                                          n_epochs=n_epochs, debug = debug, early_stopper = early_stopper, scheduler = scheduler)\n",
    "            \n",
    "    return model         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_groups(data_type):\n",
    "    if data_type in [\"Real\", \"Real_automatic\"]:\n",
    "        cross_validation_groups = [\n",
    "            [\"Crop1.csv\", \"Crop2.csv\", \"Crop3.csv\", \"Crop4.csv\"],\n",
    "            [\"Crop5_BC.csv\", \"Crop6_BC.csv\"],\n",
    "            [\"Crop7_BC.csv\", \"Crop8_BC.csv\"]\n",
    "        ]\n",
    "    else:\n",
    "        cross_validation_groups = \"even\"\n",
    "    return cross_validation_groups\n",
    "\n",
    "def get_job_params_dl(debug = False):\n",
    "    combinations = {\n",
    "        #Select type of training data here, \"Real\" for manually annotated data, and the remaining options are for synthetic data.\n",
    "        #Select single option or multiple options for multiple combinations of training and testing data.\n",
    "         \"data_type_train\":[\n",
    "                   \"Real\",\n",
    "        #\"../data/synthetic_algo_100_points\",\n",
    "        #\"../data/synthetic_algo_200_points\",\n",
    "        #\"../data/synthetic_algo_300_points\",\n",
    "        #\"../data/synthetic_algo_400_points\",\n",
    "        #\"../data/synthetic_algo_500_points\",\n",
    "        #\"../data/synthetic_algo_600_points\",\n",
    "        #\"../data/synthetic_algo_700_points\",\n",
    "        #\"../data/synthetic_algo_800_points\",\n",
    "        #\"../data/synthetic_algo_900_points\",\n",
    "        #\"../data/synthetic_algo_1000_points\"\n",
    "        ],\n",
    "        #Select type of testing data here, \"Real\" for manually annotated data, \"Real_automatic\" for data with centroids detected automatically by CNN model and the remaining options are for synthetic data.\n",
    "        #Select single option or multiple options for multiple combinations of training and testing data.\n",
    "        \"data_type_test\":[\n",
    "            #\"Real\",\n",
    "            \"Real_automatic\",\n",
    "       #\"../data/synthetic_algo_100_points\",\n",
    "        #\"../data/synthetic_algo_200_points\",\n",
    "        #\"../data/synthetic_algo_300_points\",\n",
    "        #\"../data/synthetic_algo_400_points\",\n",
    "        #\"../data/synthetic_algo_500_points\",\n",
    "        #\"../data/synthetic_algo_600_points\",\n",
    "        #\"../data/synthetic_algo_700_points\",\n",
    "        #\"../data/synthetic_algo_800_points\",\n",
    "         #\"../data/synthetic_algo_900_points\",\n",
    "        #\"../data/synthetic_algo_1000_points\"\n",
    "        ],\n",
    "        \"model_type\":[\n",
    "                #\"GNN_Classifier_Recurrent_WithoutNodeFeats\",\n",
    "                #\"GNN_Classifier_NonRecurrent_WithoutNodeFeats\",\n",
    "                \"GNN_Classifier_Recurrent_WithNodeFeats\",\n",
    "                \"GNN_Classifier_NonRecurrent_WithNodeFeats\",\n",
    "                    \"MLP\",\n",
    "                    ],\n",
    "        \"knn_inter_nodes\":[\n",
    "                           #7,\n",
    "                            10,\n",
    "                            #\"min\"\n",
    "                        ],\n",
    "        \"knn_inter_nodes_max\": [7],\n",
    "       \"knn_intra_nodes\":[0],\n",
    "        \"normalize\":[True],#[False]\n",
    "        \"node_feats\":[\n",
    "        #[\n",
    "        #    'Y', \n",
    "        #    'X', \n",
    "        #    'Z', \n",
    "        #    'node_type', \n",
    "            #'ID'\n",
    "        #],\n",
    "        [\n",
    "            'Y', \n",
    "            'X', \n",
    "            'Z', \n",
    "            'node_type',\n",
    "        ]\n",
    "        ],\n",
    "\n",
    "        \"edge_feats\":[\n",
    "            [\n",
    "     'delta_x',\n",
    "     'delta_y',\n",
    "     'delta_z',\n",
    "     'weight',\n",
    "     'angle_orientation_theta',\n",
    "     'angle_orientation_phi'],\n",
    "           [\n",
    "     'delta_x',\n",
    "     'delta_y',\n",
    "     'delta_z',\n",
    "     'weight',\n",
    "     ],\n",
    "    # [\n",
    "    #'delta_x',\n",
    "    # 'delta_y',\n",
    "    # 'delta_z',\n",
    "    # 'weight',\n",
    "    # 'x1',\n",
    "    # 'y1',\n",
    "    # 'z1',\n",
    "    # 'node_type1',\n",
    "    # 'x2',\n",
    "    # 'y2',\n",
    "    # 'z2',\n",
    "    # 'node_type2'\n",
    "    # ],\n",
    "    # [\n",
    "    #'delta_x',\n",
    "    # 'delta_y',\n",
    "    # 'delta_z',\n",
    "    # 'weight',\n",
    "    # 'x1',\n",
    "    # 'y1',\n",
    "    # 'z1',\n",
    "    # 'node_type1',\n",
    "    # 'x2',\n",
    "    # 'y2',\n",
    "    # 'z2',\n",
    "    # 'node_type2',\n",
    "    # 'angle_orientation_theta',\n",
    "    # 'angle_orientation_phi'\n",
    "    #],\n",
    "      #[\n",
    "     #'angle_orientation_theta',\n",
    "     #'angle_orientation_phi']\n",
    "     ],\n",
    "\n",
    "        \"to_undirected\":[False],\n",
    "       \"lr\":[1e-3],\n",
    "       \"n_epochs\":[100],\n",
    "        \"early_stopper\": [None],\n",
    "        \"scheduler\" : [None],\n",
    "        \"pos_weight\" : [True],\n",
    "        \"criterion\" : [\"BCEWithLogitsLoss\"],\n",
    "        \"device\" : [\"cpu\"]\n",
    "    }\n",
    "    \n",
    "    jobs = []\n",
    "    \n",
    "    # Generate all possible combinations of the dictionary values\n",
    "    for values in itertools.product(*combinations.values()):\n",
    "        # Generate a dictionary for the combination of values\n",
    "        job_dict = dict(zip(combinations.keys(), values))\n",
    "        job_dict[\"scale_features\"] = True if \"Real\" in job_dict[\"data_type_train\"] else False\n",
    "\n",
    "        index_train = \"all\"\n",
    "        index_test = \"all\"\n",
    "        cross_validation_groups_train = get_cv_groups(job_dict[\"data_type_train\"])\n",
    "        cross_validation_groups_test = get_cv_groups(job_dict[\"data_type_test\"])\n",
    "\n",
    "        job_dict[\"index_train\"] = index_train\n",
    "        job_dict[\"index_test\"] = index_test\n",
    "        job_dict[\"cross_validation_groups_train\"] = cross_validation_groups_train\n",
    "        job_dict[\"cross_validation_groups_test\"] = cross_validation_groups_test\n",
    "\n",
    "        jobs.append(job_dict)\n",
    "    \n",
    "    if(debug):\n",
    "        print(\"Total Number of jobs is:\",len(jobs))\n",
    "        print(json.dumps(jobs))\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb70f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_list_dl(jobs, debug = False):\n",
    "    #build dataframes\n",
    "    graph_list_dict_deep_learning = {}\n",
    "\n",
    "    for params in tqdm(jobs):\n",
    "\n",
    "        params_list_train = [params[\"data_type_train\"], params[\"knn_inter_nodes\"], params[\"knn_intra_nodes\"], \n",
    "                                        params[\"knn_inter_nodes_max\"], params[\"normalize\"],\n",
    "                                        params[\"scale_features\"], str(params[\"node_feats\"]), str(params[\"edge_feats\"])]\n",
    "        params_list_train = [str(param_) for param_ in params_list_train]\n",
    "        graph_key = \"_\".join(params_list_train)\n",
    "\n",
    "        if graph_key not in graph_list_dict_deep_learning:\n",
    "            graph_list = get_graph_list(params[\"data_type_train\"], params[\"knn_inter_nodes\"], params[\"knn_intra_nodes\"], \n",
    "                                            params[\"knn_inter_nodes_max\"],  normalize = params[\"normalize\"],\n",
    "                                            scale_feats = params[\"scale_features\"],\n",
    "                                            node_feats = params[\"node_feats\"], edge_feats = params[\"edge_feats\"],\n",
    "                                            shuffle = False)\n",
    "            graph_list_dict_deep_learning[graph_key] = graph_list\n",
    "\n",
    "        params_list_test = [params[\"data_type_test\"], params[\"knn_inter_nodes\"], params[\"knn_intra_nodes\"], \n",
    "                                        params[\"knn_inter_nodes_max\"], params[\"normalize\"],\n",
    "                                        params[\"scale_features\"], str(params[\"node_feats\"]), str(params[\"edge_feats\"])]\n",
    "        params_list_test = [str(param_) for param_ in params_list_test]\n",
    "        graph_key = \"_\".join(params_list_test)\n",
    "\n",
    "        if graph_key not in graph_list_dict_deep_learning:\n",
    "            graph_list = get_graph_list(params[\"data_type_test\"], params[\"knn_inter_nodes\"], params[\"knn_intra_nodes\"], \n",
    "                                            params[\"knn_inter_nodes_max\"], normalize = params[\"normalize\"],\n",
    "                                            scale_feats = params[\"scale_features\"],\n",
    "                                            node_feats = params[\"node_feats\"], edge_feats = params[\"edge_feats\"],\n",
    "                                            shuffle = False)\n",
    "            graph_list_dict_deep_learning[graph_key] = graph_list\n",
    "    return graph_list_dict_deep_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dl(graph_list_dict_deep_learning, jobs, debug = False):\n",
    "    results_list_pytorch = []\n",
    "    models_list = []\n",
    "\n",
    "    for i, job_parameters in tqdm(enumerate(jobs), total=len(jobs)):\n",
    "\n",
    "        k_inter = job_parameters[\"knn_inter_nodes\"]\n",
    "        k_inter_max = job_parameters[\"knn_inter_nodes_max\"]\n",
    "        k_intra = job_parameters[\"knn_intra_nodes\"]\n",
    "\n",
    "        scale_feats = job_parameters[\"scale_features\"]\n",
    "\n",
    "        #get data\n",
    "        params_list_train = [job_parameters[\"data_type_train\"], job_parameters[\"knn_inter_nodes\"], job_parameters[\"knn_intra_nodes\"], \n",
    "                                        job_parameters[\"knn_inter_nodes_max\"], job_parameters[\"normalize\"],\n",
    "                                        job_parameters[\"scale_features\"],\n",
    "                                         str(job_parameters[\"node_feats\"]), str(job_parameters[\"edge_feats\"])]\n",
    "        params_list_train = [str(param_) for param_ in params_list_train]\n",
    "        graph_key = \"_\".join(params_list_train)\n",
    "        graph_list_train = graph_list_dict_deep_learning[graph_key]\n",
    "\n",
    "        params_list_test = [job_parameters[\"data_type_test\"], job_parameters[\"knn_inter_nodes\"], job_parameters[\"knn_intra_nodes\"], \n",
    "                                        job_parameters[\"knn_inter_nodes_max\"], job_parameters[\"normalize\"],\n",
    "                                        job_parameters[\"scale_features\"],\n",
    "                                         str(job_parameters[\"node_feats\"]), str(job_parameters[\"edge_feats\"])]\n",
    "        params_list_test = [str(param_) for param_ in params_list_test]\n",
    "        graph_key = \"_\".join(params_list_test)\n",
    "        graph_list_test = graph_list_dict_deep_learning[graph_key]\n",
    "\n",
    "        job_parameters[\"num_classes\"] = 1\n",
    "        job_parameters[\"num_node_features\"] = graph_list_train[0].pyg_graph.x.shape[1] if graph_list_train[0].pyg_graph.x.shape[0] >0 else 0\n",
    "        job_parameters[\"num_edge_features\"] = graph_list_train[0].pyg_graph.edge_attr.shape[1]\n",
    "        job_parameters[\"num_total_features\"] = graph_list_train[0].edge_x.shape[1]\n",
    "\n",
    "        indexes_train = job_parameters[\"index_train\"]\n",
    "        indexes_test = job_parameters[\"index_test\"]\n",
    "        cross_validation_groups_train = job_parameters.get(\"cross_validation_groups_train\",[])\n",
    "        cross_validation_groups_test = job_parameters.get(\"cross_validation_groups_test\",[])\n",
    "\n",
    "        if(indexes_train==\"all\"):\n",
    "            indexes_train = [g.graph_id for g in graph_list_train]\n",
    "        if(indexes_test==\"all\"):\n",
    "            indexes_test = [g.graph_id for g in graph_list_test]\n",
    "        if(cross_validation_groups_train==\"even\"):\n",
    "            number_cross_validation_groups = 3\n",
    "            cross_validation_groups_train = distribute_elements_to_lists(indexes_train, number_cross_validation_groups)\n",
    "        if(cross_validation_groups_test==\"even\"):\n",
    "            number_cross_validation_groups = 3\n",
    "            cross_validation_groups_test = distribute_elements_to_lists(indexes_test, number_cross_validation_groups)\n",
    "\n",
    "        indexes_train = set(indexes_train)\n",
    "        indexes_test = set(indexes_test)\n",
    "\n",
    "        cv_dataset_list = []\n",
    "        if(not cross_validation_groups_train):#without cross-validation\n",
    "            graph_list_train = [el for el in graph_list_train if el.graph_id in indexes_train]\n",
    "            graph_list_test = [el for el in graph_list_test if el.graph_id in indexes_test]\n",
    "            cv_dataset_list.append({\"train\":graph_list_train, \"test\":graph_list_test})\n",
    "\n",
    "        else:#with cross-valudation\n",
    "            for i in range(len(cross_validation_groups_test)):\n",
    "                graph_list_test_cv = [el for el in graph_list_test if el.graph_id in set(cross_validation_groups_test[i])]\n",
    "                graph_list_train_cv = []\n",
    "                for j in range(len(cross_validation_groups_train)):\n",
    "                    if(j!=i):\n",
    "                        graph_list_train_cv.extend([el for el in graph_list_train if el.graph_id in set(cross_validation_groups_train[j])])\n",
    "\n",
    "                cv_dataset_list.append({\"train\": graph_list_train_cv, \"test\": graph_list_test_cv})    \n",
    "\n",
    "        #Train Model\n",
    "        results = {\"cv_results\":[], \"job_parameters\":job_parameters, \"aggregated_metrics\" : None}\n",
    "        models_list_cv = []\n",
    "        for dataset in cv_dataset_list:\n",
    "\n",
    "            result = {}\n",
    "            graph_list_train, graph_list_test = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "            model_type = job_parameters[\"model_type\"]\n",
    "            model = build_model(model_type, job_parameters[\"num_node_features\"], job_parameters[\"num_edge_features\"], \n",
    "                                job_parameters[\"num_total_features\"], 1)\n",
    "\n",
    "            model = schedule_training_GNN(job_parameters, model, \n",
    "                                               graph_list_train, graph_list_test, debug = debug)\n",
    "            \n",
    "            result[\"graphs\"] = {}\n",
    "            result[\"graphs\"][\"train\"] = graph_list_train\n",
    "            result[\"graphs\"][\"test\"] = graph_list_test\n",
    "\n",
    "            #Eval Model\n",
    "            result[\"eval\"] = eval_link_predictor(model, graph_list_train, graph_list_test, \n",
    "                                                     criterion = None,  apply_constraints = True,\n",
    "                                                    plot_roc_curve = False, debug = False)\n",
    "            results[\"cv_results\"].append(result)\n",
    "            models_list_cv.append(ModelWrapper(model, job_parameters))\n",
    "        models_list.append(models_list_cv)\n",
    "\n",
    "        #aggregate all metrics\n",
    "\n",
    "        all_metrics = {}\n",
    "        for item in results[\"cv_results\"]:\n",
    "            individual_metrics = item[\"eval\"][\"individual_metrics\"]\n",
    "            for individual_graph in individual_metrics:\n",
    "                all_metrics[individual_graph] = individual_metrics[individual_graph]\n",
    "\n",
    "        results[\"aggregated_metrics\"]  = aggregate_metrics_all(list(all_metrics.values()))\n",
    "\n",
    "        results_list_pytorch.append(results)\n",
    "    return results_list_pytorch, models_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca17617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_dl(results_list_pytorch):\n",
    "    plot_df_pytorch = plot_table(results_list_pytorch, metrics_dict_entries = [[\"@best\",\"metrics\"],[\"@best\",\"@constraints\",\"metrics\"], \n",
    "                                                                               [\"@constraints\",\"metrics\"], [\"@constraints_opt\",\"metrics\"]])\n",
    "    plot_df_pytorch = plot_df_pytorch.sort_values(by=[\"Algorithm\", \"Normalize\", \"K Inter\", 'Data Train', 'Data Test','Constraints'])\n",
    "    display(plot_df_pytorch)\n",
    "    plot_df_pytorch = plot_df_pytorch.drop([\"Data Train\", \"Data Test\"], axis=1)\n",
    "    display(plot_df_pytorch)\n",
    "    print(plot_df_to_latex(plot_df_pytorch))\n",
    "    return plot_df_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_dl = get_job_params_dl()\n",
    "\n",
    "#Apply custom processing, make sure data_type_train==data_type_test, useful for some cases namely when using synthetic data, to only allow to train and test on the same type of data.\n",
    "#jobs_dl = list(filter(lambda x: x[\"data_type_train\"]==x[\"data_type_test\"], jobs_dl))\n",
    "\n",
    "assert len(jobs_dl)>0\n",
    "\n",
    "len(jobs_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list_dl = get_graph_list_dl(jobs_dl, debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea890e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_dl, models_list = train_dl(graph_list_dl, jobs_dl, debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e26bdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "output_df = plot_results_dl(results_list_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096dda6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_final_format(output_df):\n",
    "    output_df = output_df.drop([\"Node Feat.\", \"Scale\", \"Normalize\", \"K Intra\",  \"TP Percent\",\"TP Total Count\",\"TP\",\"FP\",\"TN\",\"FN\"], axis = 1)\n",
    "    output_df = output_df.rename(columns={\"K Inter\": \"K\", \"Edge Feat.\":\"Angles\"})\n",
    "    output_df[\"Angles\"] = output_df[\"Angles\"].apply(lambda x: any(\"angle\" in item for item in x))\n",
    "    output_df = output_df[[\"Algorithm\",\"Constraints\",\"Angles\",\"K\",\"ROC AUC Score\",\"Accuracy\",\"TPR\",\"FPR\",\"Precision\",\"F1-Score\"]]\n",
    "    return output_df\n",
    "\n",
    "final_output_df = convert_to_final_format(output_df)\n",
    "print(plot_df_to_latex(final_output_df))\n",
    "final_output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2986fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_csv(array : np.array, csv_path : str, separator = \",\", columns_order=['YN', 'XN', 'YG', 'XG', 'ZN', 'ZG']):\n",
    "    with open(csv_path, 'w') as fp:\n",
    "        for i in range(len(array)):\n",
    "            row_array = array[i]\n",
    "            array_dict = {}\n",
    "            array_dict[\"XN\"], array_dict[\"YN\"], array_dict[\"ZN\"], array_dict[\"XG\"], array_dict[\"YG\"], array_dict[\"ZG\"] = row_array[0], row_array[1], row_array[2], row_array[3], row_array[4], row_array[5]\n",
    "            new_array = [array_dict[col] for col in columns_order]\n",
    "            row = separator.join(str(v) for v in new_array)\n",
    "            if(i!=(len(array)-1)):#only write \\n up to the line before the last line\n",
    "                row+=\"\\n\"\n",
    "            fp.write(row)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e7c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(output_folder, results_list_pytorch):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    results_count = 0\n",
    "    #Save results to file\n",
    "    for results_entry in results_list_pytorch:\n",
    "\n",
    "        results_entry_output_folder = os.path.join(output_folder, \"Results_\"+str(results_count))\n",
    "        if not os.path.exists(results_entry_output_folder):\n",
    "            os.makedirs(results_entry_output_folder)\n",
    "        results_info = {k:results_entry[k] for k in results_entry if k!=\"cv_results\"}\n",
    "        print(results_info[\"aggregated_metrics\"].keys())\n",
    "        desc_file_path = os.path.join(results_entry_output_folder, \"params.json\")\n",
    "        with open(desc_file_path, 'w') as f:\n",
    "            json.dump(results_info, f, indent = 2)\n",
    "\n",
    "        results_entry_output_folder_constraints = os.path.join(output_folder, \"Results_\"+str(results_count)+\"_constraints\")\n",
    "        if not os.path.exists(results_entry_output_folder_constraints):\n",
    "            os.makedirs(results_entry_output_folder_constraints)\n",
    "        desc_file_path = os.path.join(results_entry_output_folder_constraints, \"params.json\")\n",
    "        with open(desc_file_path, 'w') as f:\n",
    "            json.dump(results_info, f, indent = 2)\n",
    "\n",
    "        results_entry_output_folder_constraints_thresh = os.path.join(output_folder, \"Results_\"+str(results_count)+\"_constraints_threshold\")\n",
    "        if not os.path.exists(results_entry_output_folder_constraints_thresh):\n",
    "            os.makedirs(results_entry_output_folder_constraints_thresh)\n",
    "        desc_file_path = os.path.join(results_entry_output_folder_constraints_thresh, \"params.json\")\n",
    "        with open(desc_file_path, 'w') as f:\n",
    "            json.dump(results_info, f, indent = 2)\n",
    "\n",
    "        results_entry_output_folder_constraints_opt = os.path.join(output_folder, \"Results_\"+str(results_count)+\"_constraints_opt\")\n",
    "        if not os.path.exists(results_entry_output_folder_constraints_opt):\n",
    "            os.makedirs(results_entry_output_folder_constraints_opt)\n",
    "        desc_file_path = os.path.join(results_entry_output_folder_constraints_opt, \"params.json\")\n",
    "        with open(desc_file_path, 'w') as f:\n",
    "            json.dump(results_info, f, indent = 2)\n",
    "        \n",
    "        data_type_train = results_entry[\"job_parameters\"][\"data_type_train\"]\n",
    "        data_type_test = results_entry[\"job_parameters\"][\"data_type_test\"]\n",
    "        model_type = results_entry[\"job_parameters\"][\"model_type\"]\n",
    "        k_intra = results_entry[\"job_parameters\"][\"knn_intra_nodes\"]\n",
    "        k_inter = results_entry[\"job_parameters\"][\"knn_inter_nodes\"]\n",
    "        node_feats_str = str(results_entry[\"job_parameters\"][\"node_feats\"]).replace(\"[\",\"\").replace(\",\",\"_\").replace(\"]\",\"\").replace(\"\\'\",\"\")\n",
    "        edge_feats_str = str(results_entry[\"job_parameters\"][\"edge_feats\"]).replace(\"[\",\"\").replace(\",\",\"_\").replace(\"]\",\"\").replace(\"\\'\",\"\")\n",
    "\n",
    "        print(\"K_Intra\", k_intra, \"K_Inter\", k_inter)\n",
    "\n",
    "        print(\"Aggregated Metrics\", json.dumps(results_entry[\"aggregated_metrics\"], indent = 1, cls = CustomEncoder))\n",
    "\n",
    "        for cv_crop_index, cv_crop in enumerate(results_entry[\"cv_results\"]):\n",
    "            graphs_list_test = cv_crop[\"graphs\"][\"test\"]\n",
    "            graphs_list_train = cv_crop[\"graphs\"][\"train\"]\n",
    "            graph_individual_metrics = cv_crop[\"eval\"][\"individual_metrics\"]\n",
    "            graphs_indexes = {\"train\":[g.graph_id for g in graphs_list_train], \"test\":[g.graph_id for g in graphs_list_test]}\n",
    "\n",
    "            print(\"##############################################################\\n\")\n",
    "            print(\"CV Crop Index\",cv_crop_index, \"\", graphs_indexes)\n",
    "\n",
    "            for graph_id in graphs_indexes[\"test\"]:\n",
    "                graph_matches = [g for g in graphs_list_test if g.graph_id ==graph_id]\n",
    "                if len(graph_matches)!=1:\n",
    "                    raise ValueError(\"Multiple graphs with same ID!\")\n",
    "                graph_test = graph_matches[0]\n",
    "                graph_metrics = graph_individual_metrics[graph_id]\n",
    "\n",
    "                edge_list = GraphInfo.edge_index_to_edge_list(graph_test.pyg_graph.edge_index)\n",
    "                edge_df = GraphInfo.edge_list_to_edge_df(edge_list)\n",
    "                edge_df[\"edge_label\"] =  graph_metrics[\"@best\"][\"pred_labels\"]\n",
    "                edges_array =  pred_df_to_csv(edge_df, graph_test.nodes_df_original)\n",
    "                output_file_path = os.path.join(results_entry_output_folder, graph_test.graph_id)\n",
    "                array_to_csv(edges_array, output_file_path)\n",
    "                #nodes_df = graph_test.nodes_df\n",
    "\n",
    "                edge_df_constraints = edge_df.copy()\n",
    "                edge_df_constraints[\"edge_label\"] = graph_metrics[\"@constraints\"][\"pred_labels\"]            \n",
    "                constraints_array = pred_df_to_csv(edge_df_constraints, graph_test.nodes_df_original)\n",
    "                output_file_path = os.path.join(results_entry_output_folder_constraints, graph_test.graph_id)\n",
    "                array_to_csv(constraints_array, output_file_path)\n",
    "\n",
    "                edge_df_constraints = edge_df.copy()\n",
    "                edge_df_constraints[\"edge_label\"] = graph_metrics[\"@best\"][\"@constraints\"][\"pred_labels\"]            \n",
    "                constraints_array = pred_df_to_csv(edge_df_constraints, graph_test.nodes_df_original)\n",
    "                output_file_path = os.path.join(results_entry_output_folder_constraints_thresh, graph_test.graph_id)\n",
    "                array_to_csv(constraints_array, output_file_path)\n",
    "\n",
    "                edge_df_constraints = edge_df.copy()\n",
    "                edge_df_constraints[\"edge_label\"] = graph_metrics[\"@constraints_opt\"][\"pred_labels\"]            \n",
    "                constraints_array = pred_df_to_csv(edge_df_constraints, graph_test.nodes_df_original)\n",
    "                output_file_path = os.path.join(results_entry_output_folder_constraints_opt, graph_test.graph_id)\n",
    "                array_to_csv(constraints_array, output_file_path)\n",
    "\n",
    "                \n",
    "        results_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8076f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(os.path.join(\"./results\",\"results_real_test\"), results_list_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment this to save models\n",
    "\"\"\"\n",
    "def save_models(models_list, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    for i, models_cv_list in enumerate(models_list):\n",
    "        print(\"####\")\n",
    "        for cv_crop, model in enumerate(models_cv_list):\n",
    "            print(model.params)\n",
    "            model_name = str(i)+\"_cv-\"+str(cv_crop)+\"_\"+model.params[\"model_type\"]+\"_K-\"+str(model.params[\"knn_inter_nodes\"])+\"_Angles-\"+str(any('angle' in element for element in model.params))\n",
    "            model_path = os.path.join(output_folder, model_name) \n",
    "            model.save(model_path)\n",
    "\n",
    "save_models(models_list, os.path.join(\"./models\",\"models_test\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f4dea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
